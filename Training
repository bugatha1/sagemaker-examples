
where to find all existing aws prebuiltin container images

example of sagemaker inbuilt xgboost uri

Train:

step1 : container = sagemaker.image_uris.retrieve("xgboost", sess.boto_region_name, "1.5-1")
        display(container)


step2 : s3_input_train = TrainingInput(
            s3_data="s3://{}/{}/train".format(bucket, prefix), content_type="csv"
          )
        s3_input_validation = TrainingInput(
            s3_data="s3://{}/{}/validation/".format(bucket, prefix), content_type="csv"
        )

step3:  
        sess = sagemaker.Session()

        xgb = sagemaker.estimator.Estimator(
            container,
            role,
            instance_count=1,
            instance_type="ml.m4.xlarge",
            output_path="s3://{}/{}/output".format(bucket, prefix),
            sagemaker_session=sess,
        )
        xgb.set_hyperparameters(
            max_depth=5,
            eta=0.2,
            gamma=4,
            min_child_weight=6,
            subsample=0.8,
            verbosity=0,
            objective="binary:logistic",
            num_round=100,
        )

        xgb.fit({"train": s3_input_train, "validation": s3_input_validation})
        
 step4 : Hosting
      
        xgb_predictor = xgb.deploy(
            initial_instance_count=1, instance_type="ml.m4.xlarge", serializer=CSVSerializer()
        )
        
 step5 :  Evaluate
 
        def predict(data, rows=500):
              split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))
              predictions = ""
              for array in split_array:
                  predictions = ",".join([predictions, xgb_predictor.predict(array).decode("utf-8")])

              return np.fromstring(predictions[1:], sep=",")


          predictions = predict(test_data.to_numpy()[:, 1:])
          
          
  step 6: Clean-up
          xgb_predictor.delete_endpoint()
      
